{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai.vision import *\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_data = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py  large_size  small_size\r\n"
     ]
    }
   ],
   "source": [
    "!ls {path_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utilty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def open_PIL(path, load=False):\n",
    "    if path.suffix == '.raw':\n",
    "        img = np.fromfile(path, dtype='uint8')\n",
    "        sz = int(np.sqrt(img.size/3))\n",
    "        img = PIL.Image.fromarray(img.reshape(sz, sz, 3))\n",
    "    else:\n",
    "        img = PIL.Image.open(path)\n",
    "        \n",
    "    if load:\n",
    "        img.load()\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FastNet(nn.Module):\n",
    "    def __init__(self, num_cl):\n",
    "        super(FastNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3, stride = 2)\n",
    "        self.fc = nn.Linear(64, num_cl)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PathFolderDataset(Dataset):\n",
    "    # Loads images on demand\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.subsamples = subsamples\n",
    "        self.samples = self._get_samples()\n",
    "                       \n",
    "    def _get_samples(self):\n",
    "        samples = [(p, int(p.parent.stem)) for p in self.path_root.glob('*/*')]\n",
    "        \n",
    "        if self.subsamples is not None:\n",
    "            samples = [samples[i] for i in np.random.choice(len(samples), self.subsamples, replace=False)]\n",
    "            \n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        img = open_PIL(path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    # Preloads everything as PIL image\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.subsamples = subsamples\n",
    "        self.samples = self._get_samples()\n",
    "               \n",
    "    def _get_samples(self):\n",
    "        samples = [(p, int(p.parent.stem)) for p in self.path_root.glob('*/*')]\n",
    "        \n",
    "        if self.subsamples is not None:\n",
    "            samples = [samples[i] for i in np.random.choice(len(samples), self.subsamples, replace=False)]\n",
    "            \n",
    "        samples = [(open_PIL(s[0], load=True), s[1]) for s in samples]\n",
    "            \n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.samples[idx]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(dl, model, loss, opt, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in dl:\n",
    "            # Zero gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = model(X)\n",
    "            \n",
    "            # Loss\n",
    "            l = loss(y_hat, y)\n",
    "            \n",
    "            # Step\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # print statistics\n",
    "        print(f'Epoch: {epoch}; Loss: {l.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No augmentation\n",
    "tfms_na = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                   (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "tfms_aug = transforms.Compose([transforms.RandomAffine(10, \n",
    "                                                       translate=(0.1, 0.1), \n",
    "                                                       scale=(0.9, 1.1), \n",
    "                                                       shear=(-5, 5), \n",
    "                                                       resample=PIL.Image.BICUBIC),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                    (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test small image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# small size, path, no augmentation, format\n",
    "path_root = path_data/'small_size'\n",
    "ds_ss_path_na_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_na)\n",
    "ds_ss_path_na_tif_dataset = PathFolderDataset(path_root/'tif', transforms=tfms_na)\n",
    "ds_ss_path_na_jpg_dataset = PathFolderDataset(path_root/'jpg', transforms=tfms_na)\n",
    "ds_ss_path_na_raw_dataset = PathFolderDataset(path_root/'raw', transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# small size, path, augmentation, format\n",
    "path_root = path_data/'small_size'\n",
    "ds_ss_path_aug_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# small size, image, no augmentation, format\n",
    "path_root = path_data/'small_size'\n",
    "ds_ss_img_na_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# small size, image, augmentation, format\n",
    "path_root = path_data/'small_size'\n",
    "ds_ss_img_aug_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 0.04181664064526558\n",
      "CPU times: user 14min 17s, sys: 9.3 s, total: 14min 26s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=1, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Very slow, lets increase batch size and also test other file formats which should show encoding speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3132524490356445\n",
      "CPU times: user 8.47 s, sys: 400 ms, total: 8.87 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=256, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2152068614959717\n",
      "CPU times: user 8.44 s, sys: 278 ms, total: 8.72 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_tif_dataset, batch_size=256, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.264390230178833\n",
      "CPU times: user 8.49 s, sys: 326 ms, total: 8.82 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_jpg_dataset, batch_size=256, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.348086357116699\n",
      "CPU times: user 8.34 s, sys: 410 ms, total: 8.75 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_raw_dataset, batch_size=256, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "tiff is slow for some reason... jpg is slow for obvious reasons. png and raw appear to be about the same; increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.324636459350586\n",
      "CPU times: user 8.13 s, sys: 801 ms, total: 8.93 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=256, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Workers help out a lot; try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3409769535064697\n",
      "CPU times: user 8.79 s, sys: 812 ms, total: 9.6 s\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_aug_png_dataset, batch_size=256, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A little slower, now try doing in-memory dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.151681661605835\n",
      "CPU times: user 8.7 s, sys: 834 ms, total: 9.53 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_img_na_png_dataset, batch_size=256, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Faster, but then you have to store everything in memory. Try in memory with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.399033308029175\n",
      "CPU times: user 7.83 s, sys: 737 ms, total: 8.56 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_img_aug_png_dataset, batch_size=256, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Delete in memory dataloader to free up some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del dl, ds_ss_img_na_png_dataset, ds_ss_img_aug_png_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test large image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large size, path, no augmentation, format\n",
    "path_root = path_data/'large_size'\n",
    "ds_ls_path_na_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_na, subsamples=2000)\n",
    "ds_ls_path_na_tif_dataset = PathFolderDataset(path_root/'tif', transforms=tfms_na, subsamples=2000)\n",
    "ds_ls_path_na_jpg_dataset = PathFolderDataset(path_root/'jpg', transforms=tfms_na, subsamples=2000)\n",
    "ds_ls_path_na_raw_dataset = PathFolderDataset(path_root/'raw', transforms=tfms_na, subsamples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large size, path, augmentation, format\n",
    "path_root = path_data/'large_size'\n",
    "ds_ls_path_aug_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_aug, subsamples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large size, image, no augmentation, format\n",
    "path_root = path_data/'large_size'\n",
    "ds_ls_img_na_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_na, subsamples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large size, image, augmentation, format\n",
    "path_root = path_data/'large_size'\n",
    "ds_ls_img_aug_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_aug, subsamples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test large image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.319605588912964\n",
      "CPU times: user 2min 27s, sys: 4.31 s, total: 2min 32s\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_na_png_dataset, batch_size=1, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size and also test other file formats which should show encoding speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3001198768615723\n",
      "CPU times: user 1min 46s, sys: 50.1 s, total: 2min 36s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_na_png_dataset, batch_size=32, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.363144874572754\n",
      "CPU times: user 1min 58s, sys: 52.4 s, total: 2min 50s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_na_tif_dataset, batch_size=32, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2818691730499268\n",
      "CPU times: user 1min 47s, sys: 49.9 s, total: 2min 36s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_na_jpg_dataset, batch_size=32, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3126261234283447\n",
      "CPU times: user 1min 48s, sys: 50.9 s, total: 2min 39s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_na_raw_dataset, batch_size=32, num_workers=1)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding doesn't seem to have much of an effect; increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.312602996826172\n",
      "CPU times: user 1min 46s, sys: 53.8 s, total: 2min 40s\n",
      "Wall time: 34.8 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_na_png_dataset, batch_size=32, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers dont seem to help much; try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3226513862609863\n",
      "CPU times: user 1min 50s, sys: 51.1 s, total: 2min 41s\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_path_aug_png_dataset, batch_size=32, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smidge slower, now try doing in-memory dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.280503273010254\n",
      "CPU times: user 1min 43s, sys: 50.6 s, total: 2min 34s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_img_na_png_dataset, batch_size=32, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a smidge faster, but then you have to store everything in memory. Try in memory with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2880043983459473\n",
      "CPU times: user 1min 48s, sys: 48.6 s, total: 2min 36s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ls_img_aug_png_dataset, batch_size=32, num_workers=12)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems overall for larger images that batchsize/workers/in-memory doesn't have much of an effect. And, doing %prun actually shows most of the time is spent in the conv2d function and computing gradients, so that is the bottleneck vs IO/encoding/augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete in memory dataloader to free up some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dl, ds_ls_img_na_png_dataset, ds_ls_img_aug_png_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
