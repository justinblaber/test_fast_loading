{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "from IPython.core.debugger import set_trace\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py  medium_imgs  small_hdf5  small_lmdb\r\n",
      "medium_hdf5\t     medium_lmdb  small_imgs\r\n"
     ]
    }
   ],
   "source": [
    "!ls {path_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastNet(nn.Module):\n",
    "    # Just do a single convolution followed by a linear layer\n",
    "    # Made to be simple to emphasize affect of image loading\n",
    "    # and augmentation\n",
    "    def __init__(self, num_cl):\n",
    "        super(FastNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3, stride=2)\n",
    "        self.fc = nn.Linear(64, num_cl)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _open_path(path_img, load=False):\n",
    "    if path_img.suffix == '.raw':\n",
    "        img = np.fromfile(path_img, dtype='uint8')\n",
    "        sz = int(np.sqrt(img.size/3))\n",
    "        img = Image.fromarray(img.reshape(sz, sz, 3))\n",
    "    else:\n",
    "        img = Image.open(path_img)\n",
    "\n",
    "    if load:\n",
    "        img.load()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathFolderDataset(Dataset):\n",
    "    # Loads images on demand\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.subsamples = subsamples\n",
    "        self.samples = self._get_samples()\n",
    "                       \n",
    "    def _get_samples(self):\n",
    "        samples = [(p, int(p.parent.stem)) for p in self.path_root.glob('*/*')]\n",
    "        \n",
    "        if self.subsamples is not None:\n",
    "            samples = [samples[i] for i in np.random.choice(len(samples), self.subsamples, replace=False)]\n",
    "            \n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        img = _open_path(path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    # Preloads everything a pillow image\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.subsamples = subsamples\n",
    "        self.samples = self._get_samples()\n",
    "               \n",
    "    def _get_samples(self):\n",
    "        samples = [(p, int(p.parent.stem)) for p in self.path_root.glob('*/*')]\n",
    "        \n",
    "        if self.subsamples is not None:\n",
    "            samples = [samples[i] for i in np.random.choice(len(samples), self.subsamples, replace=False)]\n",
    "            \n",
    "        # Load everything; must .load() or else tons of open file pointers will cause a crash\n",
    "        samples = [(_open_path(s[0], load=True), s[1]) for s in samples]\n",
    "            \n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.samples[idx]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDBDataset(Dataset):\n",
    "    # Loads images from lmdb\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.df_meta = pd.read_csv(path_root/'meta.csv')\n",
    "        self.env = lmdb.open((path_root/'lmdb').as_posix(), readonly=True)\n",
    "        \n",
    "        if subsamples is not None:\n",
    "            self.df_meta = self.df_meta.sample(n=subsamples, replace=False) \n",
    "                              \n",
    "    def __len__(self):\n",
    "        return len(self.df_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        info = self.df_meta.iloc[idx]\n",
    "        key = info['key']\n",
    "        target = info['label']\n",
    "        \n",
    "        # NOTE: might be slow since this will do each item as separate transaction\n",
    "        with self.env.begin() as txn:\n",
    "            img = txn.get(key.encode('ascii'))\n",
    "                          \n",
    "        img = pickle.loads(img)\n",
    "        img = Image.fromarray(img)\n",
    "                \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    # Loads images from hdf5\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.df_meta = pd.read_csv(path_root/'meta.csv')\n",
    "            \n",
    "        if subsamples is not None:\n",
    "            self.df_meta = self.df_meta.sample(n=subsamples, replace=False) \n",
    "                               \n",
    "    def __len__(self):\n",
    "        return len(self.df_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        info = self.df_meta.iloc[idx]\n",
    "        target = info['label']        \n",
    "                \n",
    "        with h5py.File(path_root/'data.hdf5', 'r', libver='latest', swmr=True) as f:\n",
    "            img = f['data'][info['idx']]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl, model, loss, opt, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in dl:\n",
    "            # Send data to gpu\n",
    "            X, y = X.to(torch.device('cuda')), y.to(torch.device('cuda'))\n",
    "        \n",
    "            opt.zero_grad()    # Zero gradients\n",
    "            y_hat = model(X)   # Forward pass\n",
    "            l = loss(y_hat, y) # Loss\n",
    "            l.backward()       # Compute gradients\n",
    "            opt.step()         # Step\n",
    "\n",
    "        # print statistics\n",
    "        print(f'Epoch: {epoch}; Loss: {l.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No augmentation\n",
    "tfms_na = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                   (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "tfms_aug = transforms.Compose([transforms.ColorJitter(),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.RandomPerspective(),\n",
    "                               transforms.RandomAffine(10, \n",
    "                                                       translate=(0.1, 0.1), \n",
    "                                                       scale=(0.9, 1.1), \n",
    "                                                       shear=(-5, 5), \n",
    "                                                       resample=Image.BICUBIC),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                    (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, path, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_path_na_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_na)\n",
    "ds_ss_path_na_tif_dataset = PathFolderDataset(path_root/'tif', transforms=tfms_na)\n",
    "ds_ss_path_na_jpg_dataset = PathFolderDataset(path_root/'jpg', transforms=tfms_na)\n",
    "ds_ss_path_na_raw_dataset = PathFolderDataset(path_root/'raw', transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, path, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_path_aug_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, image, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_img_na_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, image, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_img_aug_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 3.6578400135040283\n",
      "CPU times: user 44.1 s, sys: 7.21 s, total: 51.3 s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size and also test other file formats which should show encoding speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3125243186950684\n",
      "CPU times: user 791 ms, sys: 234 ms, total: 1.02 s\n",
      "Wall time: 8.81 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.297828197479248\n",
      "CPU times: user 834 ms, sys: 230 ms, total: 1.06 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_tif_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2859106063842773\n",
      "CPU times: user 880 ms, sys: 182 ms, total: 1.06 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_jpg_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.315157413482666\n",
      "CPU times: user 828 ms, sys: 214 ms, total: 1.04 s\n",
      "Wall time: 9.81 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_raw_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3262479305267334\n",
      "CPU times: user 745 ms, sys: 534 ms, total: 1.28 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help; try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2991511821746826\n",
      "CPU times: user 628 ms, sys: 494 ms, total: 1.12 s\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_aug_png_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little slower, now try doing in-memory dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.296252489089966\n",
      "CPU times: user 671 ms, sys: 481 ms, total: 1.15 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_img_na_png_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster, but then you have to store everything in memory. Try in memory with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.286672830581665\n",
      "CPU times: user 621 ms, sys: 465 ms, total: 1.09 s\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_img_aug_png_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete in memory dataloader to free up some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dl, ds_ss_img_na_png_dataset, ds_ss_img_aug_png_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, path, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_path_na_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_na, subsamples=4000)\n",
    "ds_ms_path_na_tif_dataset = PathFolderDataset(path_root/'tif', transforms=tfms_na, subsamples=4000)\n",
    "ds_ms_path_na_jpg_dataset = PathFolderDataset(path_root/'jpg', transforms=tfms_na, subsamples=4000)\n",
    "ds_ms_path_na_raw_dataset = PathFolderDataset(path_root/'raw', transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, path, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_path_aug_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, image, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_img_na_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, image, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_img_aug_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test medium image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.503147840499878\n",
      "CPU times: user 7.67 s, sys: 1.67 s, total: 9.34 s\n",
      "Wall time: 8.1 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_png_dataset, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase batch size and also test other file formats which should show encoding speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3388760089874268\n",
      "CPU times: user 4.22 s, sys: 606 ms, total: 4.82 s\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_png_dataset, batch_size=64, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3120784759521484\n",
      "CPU times: user 4.15 s, sys: 625 ms, total: 4.77 s\n",
      "Wall time: 6.31 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_tif_dataset, batch_size=64, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2963180541992188\n",
      "CPU times: user 4.12 s, sys: 720 ms, total: 4.84 s\n",
      "Wall time: 6.28 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_jpg_dataset, batch_size=64, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2758541107177734\n",
      "CPU times: user 4.19 s, sys: 644 ms, total: 4.84 s\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_raw_dataset, batch_size=64, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding doesn't seem to have much of an effect; increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2884750366210938\n",
      "CPU times: user 3.95 s, sys: 1.6 s, total: 5.54 s\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_png_dataset, batch_size=64, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help again; try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.287156581878662\n",
      "CPU times: user 2.38 s, sys: 1.24 s, total: 3.62 s\n",
      "Wall time: 5.93 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_aug_png_dataset, batch_size=64, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much slower, now try doing in-memory dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.306854724884033\n",
      "CPU times: user 4.11 s, sys: 1.63 s, total: 5.74 s\n",
      "Wall time: 2.92 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_img_na_png_dataset, batch_size=64, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit faster. Try in memory with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.298717975616455\n",
      "CPU times: user 1.82 s, sys: 1.23 s, total: 3.04 s\n",
      "Wall time: 5.48 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_img_aug_png_dataset, batch_size=64, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete in memory dataloader to free up some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dl, ds_ms_img_na_png_dataset, ds_ms_img_aug_png_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, lmdb, no augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ss_lmdb_na_dataset = LMDBDataset(path_root, transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, lmdb, augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ss_lmdb_aug_dataset = LMDBDataset(path_root, transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.0853044986724854\n",
      "CPU times: user 43.8 s, sys: 5.96 s, total: 49.8 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_na_dataset, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.28078556060791\n",
      "CPU times: user 836 ms, sys: 235 ms, total: 1.07 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_na_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2987771034240723\n",
      "CPU times: user 663 ms, sys: 584 ms, total: 1.25 s\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_na_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help a bunch! Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.365288019180298\n",
      "CPU times: user 659 ms, sys: 656 ms, total: 1.31 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_aug_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, lmdb, no augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ms_lmdb_na_dataset = LMDBDataset(path_root, transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, lmdb, augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ms_lmdb_aug_dataset = LMDBDataset(path_root, transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test medium image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.0810186862945557\n",
      "CPU times: user 7.82 s, sys: 1.72 s, total: 9.54 s\n",
      "Wall time: 7.53 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_na_dataset, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3068225383758545\n",
      "CPU times: user 4.2 s, sys: 613 ms, total: 4.81 s\n",
      "Wall time: 4.76 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_na_dataset, batch_size=64, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3554763793945312\n",
      "CPU times: user 3.98 s, sys: 1.56 s, total: 5.54 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_na_dataset, batch_size=64, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3180508613586426\n",
      "CPU times: user 2.44 s, sys: 1.26 s, total: 3.7 s\n",
      "Wall time: 6.24 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_aug_dataset, batch_size=64, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, hdf5, no augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ss_hdf5_na_dataset = HDF5Dataset(path_root, transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, hdf5, augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ss_hdf5_aug_dataset = HDF5Dataset(path_root, transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.0694477558135986\n",
      "CPU times: user 44.3 s, sys: 6.39 s, total: 50.7 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_na_dataset, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3224947452545166\n",
      "CPU times: user 814 ms, sys: 296 ms, total: 1.11 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_na_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3096280097961426\n",
      "CPU times: user 825 ms, sys: 637 ms, total: 1.46 s\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_na_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help a bunch! Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3232290744781494\n",
      "CPU times: user 773 ms, sys: 648 ms, total: 1.42 s\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_aug_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, hdf5, no augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ms_hdf5_na_dataset = HDF5Dataset(path_root, transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, hdf5, augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ms_hdf5_aug_dataset = HDF5Dataset(path_root, transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test medium image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.048393487930298\n",
      "CPU times: user 8.74 s, sys: 2.16 s, total: 10.9 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_na_dataset, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.310324192047119\n",
      "CPU times: user 3.85 s, sys: 747 ms, total: 4.59 s\n",
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_na_dataset, batch_size=256, num_workers=1, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3100638389587402\n",
      "CPU times: user 3.69 s, sys: 1.97 s, total: 5.66 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_na_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3106980323791504\n",
      "CPU times: user 3.42 s, sys: 1.37 s, total: 4.79 s\n",
      "Wall time: 8.15 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_aug_dataset, batch_size=256, num_workers=12, shuffle=True, pin_memory=True)\n",
    "model = FastNet(10).cuda()\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
