{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "from IPython.core.debugger import set_trace\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py  medium_imgs  small_hdf5  small_lmdb\r\n",
      "medium_hdf5\t     medium_lmdb  small_imgs\r\n"
     ]
    }
   ],
   "source": [
    "!ls {path_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastNet(nn.Module):\n",
    "    # Just do a single convolution followed by a linear layer\n",
    "    # Made to be simple to emphasize affect of image loading\n",
    "    # and augmentation\n",
    "    def __init__(self, num_cl):\n",
    "        super(FastNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3, stride=2)\n",
    "        self.fc = nn.Linear(64, num_cl)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _open_path(path_img, load=False):\n",
    "    if path_img.suffix == '.raw':\n",
    "        img = np.fromfile(path_img, dtype='uint8')\n",
    "        sz = int(np.sqrt(img.size/3))\n",
    "        img = Image.fromarray(img.reshape(sz, sz, 3))\n",
    "    else:\n",
    "        img = Image.open(path_img)\n",
    "\n",
    "    if load:\n",
    "        img.load()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathFolderDataset(Dataset):\n",
    "    # Loads images on demand\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.subsamples = subsamples\n",
    "        self.samples = self._get_samples()\n",
    "                       \n",
    "    def _get_samples(self):\n",
    "        samples = [(p, int(p.parent.stem)) for p in self.path_root.glob('*/*')]\n",
    "        \n",
    "        if self.subsamples is not None:\n",
    "            samples = [samples[i] for i in np.random.choice(len(samples), self.subsamples, replace=False)]\n",
    "            \n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        img = _open_path(path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    # Preloads everything a pillow image\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.subsamples = subsamples\n",
    "        self.samples = self._get_samples()\n",
    "               \n",
    "    def _get_samples(self):\n",
    "        samples = [(p, int(p.parent.stem)) for p in self.path_root.glob('*/*')]\n",
    "        \n",
    "        if self.subsamples is not None:\n",
    "            samples = [samples[i] for i in np.random.choice(len(samples), self.subsamples, replace=False)]\n",
    "            \n",
    "        # Load everything; must .load() or else tons of open file pointers will cause a crash\n",
    "        samples = [(_open_path(s[0], load=True), s[1]) for s in samples]\n",
    "            \n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.samples[idx]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDBDataset(Dataset):\n",
    "    # Loads images from lmdb\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.df_meta = pd.read_csv(path_root/'meta.csv')\n",
    "        self.env = lmdb.open((path_root/'lmdb').as_posix(), readonly=True)\n",
    "        \n",
    "        if subsamples is not None:\n",
    "            self.df_meta = self.df_meta.sample(n=subsamples, replace=False) \n",
    "                              \n",
    "    def __len__(self):\n",
    "        return len(self.df_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        info = self.df_meta.iloc[idx]\n",
    "        key = info['key']\n",
    "        target = info['label']\n",
    "        \n",
    "        # NOTE: might be slow since this will do each item as separate transaction\n",
    "        with self.env.begin() as txn:\n",
    "            img = txn.get(key.encode('ascii'))\n",
    "                          \n",
    "        img = pickle.loads(img)\n",
    "        img = Image.fromarray(img)\n",
    "                \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    # Loads images from hdf5\n",
    "    def __init__(self, path_root, transforms=None, subsamples=None):\n",
    "        self.path_root = path_root\n",
    "        self.transforms = transforms\n",
    "        self.df_meta = pd.read_csv(path_root/'meta.csv')\n",
    "            \n",
    "        if subsamples is not None:\n",
    "            self.df_meta = self.df_meta.sample(n=subsamples, replace=False) \n",
    "                               \n",
    "    def __len__(self):\n",
    "        return len(self.df_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        info = self.df_meta.iloc[idx]\n",
    "        target = info['label']        \n",
    "                \n",
    "        with h5py.File(path_root/'data.hdf5', 'r', libver='latest', swmr=True) as f:\n",
    "            img = f['data'][info['idx']]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "                \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl, model, loss, opt, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in dl:\n",
    "            opt.zero_grad()    # Zero gradients\n",
    "            y_hat = model(X)   # Forward pass\n",
    "            l = loss(y_hat, y) # Loss\n",
    "            l.backward()       # Compute gradients\n",
    "            opt.step()         # Step\n",
    "\n",
    "        # print statistics\n",
    "        print(f'Epoch: {epoch}; Loss: {l.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No augmentation\n",
    "tfms_na = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                   (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "tfms_aug = transforms.Compose([transforms.ColorJitter(),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.RandomPerspective(),\n",
    "                               transforms.RandomAffine(10, \n",
    "                                                       translate=(0.1, 0.1), \n",
    "                                                       scale=(0.9, 1.1), \n",
    "                                                       shear=(-5, 5), \n",
    "                                                       resample=Image.BICUBIC),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                                    (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, path, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_path_na_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_na)\n",
    "ds_ss_path_na_tif_dataset = PathFolderDataset(path_root/'tif', transforms=tfms_na)\n",
    "ds_ss_path_na_jpg_dataset = PathFolderDataset(path_root/'jpg', transforms=tfms_na)\n",
    "ds_ss_path_na_raw_dataset = PathFolderDataset(path_root/'raw', transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, path, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_path_aug_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, image, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_img_na_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, image, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ss_img_aug_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.0109260082244873\n",
      "CPU times: user 10min 7s, sys: 7.41 s, total: 10min 14s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size and also test other file formats which should show encoding speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.317831516265869\n",
      "CPU times: user 27.6 s, sys: 1.26 s, total: 28.9 s\n",
      "Wall time: 9.69 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2938072681427\n",
      "CPU times: user 27.3 s, sys: 1.29 s, total: 28.6 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_tif_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.303396701812744\n",
      "CPU times: user 27.3 s, sys: 1.25 s, total: 28.6 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_jpg_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.291125774383545\n",
      "CPU times: user 27 s, sys: 1.36 s, total: 28.4 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_raw_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.311436653137207\n",
      "CPU times: user 37.8 s, sys: 1.65 s, total: 39.5 s\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_na_png_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help; try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3233230113983154\n",
      "CPU times: user 40.6 s, sys: 1.48 s, total: 42.1 s\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_path_aug_png_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little slower, now try doing in-memory dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.294036626815796\n",
      "CPU times: user 26 s, sys: 1.45 s, total: 27.5 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_img_na_png_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster, but then you have to store everything in memory. Try in memory with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.303027629852295\n",
      "CPU times: user 38 s, sys: 1.58 s, total: 39.6 s\n",
      "Wall time: 5.24 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_img_aug_png_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete in memory dataloader to free up some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dl, ds_ss_img_na_png_dataset, ds_ss_img_aug_png_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, path, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_path_na_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_na, subsamples=4000)\n",
    "ds_ms_path_na_tif_dataset = PathFolderDataset(path_root/'tif', transforms=tfms_na, subsamples=4000)\n",
    "ds_ms_path_na_jpg_dataset = PathFolderDataset(path_root/'jpg', transforms=tfms_na, subsamples=4000)\n",
    "ds_ms_path_na_raw_dataset = PathFolderDataset(path_root/'raw', transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, path, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_path_aug_png_dataset = PathFolderDataset(path_root/'png', transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, image, no augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_img_na_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, image, augmentation, format\n",
    "path_root = path_data/(size + '_imgs')\n",
    "ds_ms_img_aug_png_dataset = ImageFolderDataset(path_root/'png', transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test medium image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2764034271240234\n",
      "CPU times: user 3min 12s, sys: 2.01 s, total: 3min 14s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_png_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase batch size and also test other file formats which should show encoding speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3416311740875244\n",
      "CPU times: user 58.9 s, sys: 22.7 s, total: 1min 21s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_png_dataset, batch_size=64, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.324099063873291\n",
      "CPU times: user 58.8 s, sys: 22.9 s, total: 1min 21s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_tif_dataset, batch_size=64, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.338833808898926\n",
      "CPU times: user 58.9 s, sys: 23 s, total: 1min 21s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_jpg_dataset, batch_size=64, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.332303285598755\n",
      "CPU times: user 56.1 s, sys: 22.2 s, total: 1min 18s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_raw_dataset, batch_size=64, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding doesn't seem to have much of an effect; increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2841930389404297\n",
      "CPU times: user 53.5 s, sys: 21.3 s, total: 1min 14s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_na_png_dataset, batch_size=64, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers dont seem to help much; try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.316070795059204\n",
      "CPU times: user 59.7 s, sys: 24.9 s, total: 1min 24s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_path_aug_png_dataset, batch_size=64, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smidge slower, now try doing in-memory dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.283987045288086\n",
      "CPU times: user 54 s, sys: 22.5 s, total: 1min 16s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_img_na_png_dataset, batch_size=64, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the same speed. Try in memory with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3013663291931152\n",
      "CPU times: user 59.7 s, sys: 24.4 s, total: 1min 24s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_img_aug_png_dataset, batch_size=64, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems overall for medium images that batchsize/workers/in-memory doesn't have too much of an effect. And, doing %prun actually shows most of the time is spent in the conv2d function and computing gradients, so that is the bottleneck vs IO/encoding/augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete in memory dataloader to free up some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dl, ds_ms_img_na_png_dataset, ds_ms_img_aug_png_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, lmdb, no augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ss_lmdb_na_dataset = LMDBDataset(path_root, transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, lmdb, augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ss_lmdb_aug_dataset = LMDBDataset(path_root, transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 3.264969825744629\n",
      "CPU times: user 12min 2s, sys: 8.03 s, total: 12min 10s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_na_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3026795387268066\n",
      "CPU times: user 27.5 s, sys: 953 ms, total: 28.4 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_na_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2961742877960205\n",
      "CPU times: user 38.9 s, sys: 1.42 s, total: 40.3 s\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_na_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help a bunch! Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3149333000183105\n",
      "CPU times: user 47.3 s, sys: 1.43 s, total: 48.7 s\n",
      "Wall time: 7.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_lmdb_aug_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, lmdb, no augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ms_lmdb_na_dataset = LMDBDataset(path_root, transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, lmdb, augmentation\n",
    "path_root = path_data/(size + '_lmdb')\n",
    "ds_ms_lmdb_aug_dataset = LMDBDataset(path_root, transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test medium image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.1160240173339844\n",
      "CPU times: user 3min 20s, sys: 2.22 s, total: 3min 22s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_na_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3144407272338867\n",
      "CPU times: user 1min 3s, sys: 13.5 s, total: 1min 17s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_na_dataset, batch_size=64, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3241140842437744\n",
      "CPU times: user 1min 1s, sys: 14.5 s, total: 1min 15s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_na_dataset, batch_size=64, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.36087703704834\n",
      "CPU times: user 1min 6s, sys: 15.1 s, total: 1min 21s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_lmdb_aug_dataset, batch_size=64, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, hdf5, no augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ss_hdf5_na_dataset = HDF5Dataset(path_root, transforms=tfms_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small size, hdf5, augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ss_hdf5_aug_dataset = HDF5Dataset(path_root, transforms=tfms_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test small image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.133254051208496\n",
      "CPU times: user 15min 8s, sys: 8.43 s, total: 15min 17s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_na_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, lets increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2974672317504883\n",
      "CPU times: user 27.1 s, sys: 1.08 s, total: 28.2 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_na_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2833480834960938\n",
      "CPU times: user 50.9 s, sys: 1.44 s, total: 52.3 s\n",
      "Wall time: 8.83 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_na_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers help a bunch! Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.322622060775757\n",
      "CPU times: user 1min 3s, sys: 1.67 s, total: 1min 5s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ss_hdf5_aug_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, hdf5, no augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ms_hdf5_na_dataset = HDF5Dataset(path_root, transforms=tfms_na, subsamples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium size, hdf5, augmentation\n",
    "path_root = path_data/(size + '_hdf5')\n",
    "ds_ms_hdf5_aug_dataset = HDF5Dataset(path_root, transforms=tfms_aug, subsamples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test medium image with single batch size and single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.180546760559082\n",
      "CPU times: user 3min 31s, sys: 2.38 s, total: 3min 33s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_na_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.3224644660949707\n",
      "CPU times: user 47 s, sys: 23.8 s, total: 1min 10s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_na_dataset, batch_size=256, num_workers=1, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.2769808769226074\n",
      "CPU times: user 41.9 s, sys: 24.2 s, total: 1min 6s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_na_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workers... make it slower? Maybe some concurrency issues with hdf5? Try some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 2.288996458053589\n",
      "CPU times: user 44.3 s, sys: 23.2 s, total: 1min 7s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_ms_hdf5_aug_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "model = FastNet(10)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "%time train(dl, model, loss, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
